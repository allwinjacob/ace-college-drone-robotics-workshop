{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31b2f98-1400-40ca-a2e2-58be555fc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #to process natural language\n",
    "import numpy as np\n",
    "import random # to generte random responses to feel less repetitive\n",
    "import string # to do string manipulations, because sometimes we need to remove punctuations,need to convert it in lowercase etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b03cb-c590-456a-a147-80d39d9770ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1791962-f099-415f-a95c-2a8771a998ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Machine learning.txt', 'r', errors='ignore')\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1914670-7a5c-40f3-a5c4-2fce1605e8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine  learning is a rapidly evolving field within artificial intelligence (ai) that focuses on the development of algorithms and statistical models that enable computers to learn from and make predictions or decisions based on data.', 'unlike traditional programming where explicit instructions are given for each task, machine learning allows systems to learn patterns and relationships within data, and to improve their performance over time as they are exposed to more data.', 'this capability is grounded in the idea that systems can automatically improve their performance without human intervention through experience.', 'at its core, machine learning is divided into several categories, with the primary ones being supervised learning, unsupervised learning, and reinforcement learning.', 'each of these categories approaches the task of learning from data in a unique way, depending on the nature of the data and the desired outcome.']\n",
      "['machine', 'learning', 'is', 'a', 'rapidly']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt_tab: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    }
   ],
   "source": [
    "nltk.data.path.append('/home/nithin/nltk_data')  # Add your nltk_data path\n",
    "nltk.download('punkt_tab')\n",
    "raw=raw.lower()# convert all strings to lowercase\n",
    "nltk.download('punkt')# Punkt is a pre-trained model that helps in sentence tokenization, i.e., splitting a text into sentences\n",
    "nltk.download('wordnet')#WordNet is a large database of English words that groups words into sets of synonyms \n",
    "sentence_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)\n",
    "print(sentence_tokens[:5])\n",
    "print(word_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b7105e7-4e58-4a7f-aca6-92d074ce8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the text by removing punctuation and reducing words to their base or root form (lemmatization)\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation) ##This dictionary will be used to remove punctuation from the text.\n",
    "\n",
    "def lem_tokens(tokens): #takes a list of tokens (words) as input and returns a list of lemmatized tokens.\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def lem_normalize(text):#This function, lem_normalize, normalizes the input text by performing  translate(),lower(), tokenize()\n",
    "    return lem_tokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "# translate() used to modify a string by replacing characters based on a specified translation table\n",
    "#text.lower() converts all characters in the text to lowercase, ensuring uniformity.\n",
    "#translate(remove_punct_dict) removes any punctuation from the text using the remove_punct_dict created earlier.\n",
    "#nltk.word_tokenize() splits the cleaned text into individual words (tokens).\n",
    "#lem_tokens() is then called to lemmatize each token in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c603ec47-5fb1-442b-9a2e-c84c75e26453",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = ('hello', 'hi', 'greetings', 'sup', 'what\\'s up', 'hey',)\n",
    "GREETING_RESPONSES = ['hi', 'hey', '*nods*', 'hi there', 'hello', 'I am glad! You are talking to me']\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51665a04-da97-46d0-a42d-45d6f9100c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a response to a user's input based on the similarity between the user's query and the chatbot's available data.\n",
    "\n",
    "GREETING_INPUTS = ('hello', 'hi', 'greetings', 'sup', 'what\\'s up', 'hey',)\n",
    "GREETING_RESPONSES = ['hi', 'hey', '*nods*', 'hi there', 'hello', 'I am glad! You are talking to me']\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #TF-IDF is a statistical measure used to evaluate the importance of a \n",
    "#word in a document relative to a collection of documents (corpus). TF-IDF allows the model to focus on the most important \n",
    "#words when comparing the user input to the available data.\n",
    "from sklearn.metrics.pairwise import cosine_similarity #Cosine similarity is widely used in text processing to measure\n",
    "# the similarity between two documents (or sentences) represented as vectors\n",
    "\n",
    "\n",
    "def response(user_response):\n",
    "    robo_response = ''\n",
    "    \n",
    "    # Ensure sentence_tokens doesn't contain duplicates\n",
    "    if user_response not in sentence_tokens:\n",
    "        sentence_tokens.append(user_response)\n",
    "\n",
    "    # Check if we have more than one sentence for comparison\n",
    "    if len(sentence_tokens) > 1:\n",
    "        # Vectorizing and calculating similarities\n",
    "        vectorizer = TfidfVectorizer(tokenizer=lem_normalize, stop_words='english')\n",
    "        tfidf = vectorizer.fit_transform(sentence_tokens)  # TF-IDF matrix for all sentence tokens\n",
    "\n",
    "        values = cosine_similarity(tfidf[-1], tfidf[:-1])  # Compare the new input against all previous sentences\n",
    "        idx = values.argsort()[0][-2]  # Get the index of the second most similar sentence\n",
    "        flat = values.flatten()\n",
    "        flat.sort()\n",
    "        req_tfidf = flat[-2]  # The second highest similarity score\n",
    "        \n",
    "        if req_tfidf == 0:\n",
    "            # If similarity is too low, respond with a default message\n",
    "            robo_response = \"Sorry, I don't understand you.\"\n",
    "        else:\n",
    "            # Return the most similar sentence\n",
    "            robo_response = sentence_tokens[idx]\n",
    "    else:\n",
    "        # If only one sentence, return a default response\n",
    "        robo_response = \"Sorry, I don't understand you.\"\n",
    "\n",
    "    return robo_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9665841-6bf6-44ad-b455-25afecd70bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyttsx3 in /home/nithin/anaconda3/lib/python3.12/site-packages (2.98)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecccf2b4-4eef-49d7-ab8c-97d272b0d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "# Set the rate (speed) of speech\n",
    "engine.setProperty('rate', 150)  # 150 words per minute\n",
    "\n",
    "# Set the volume (0.0 to 1.0)\n",
    "engine.setProperty('volume', 0.9)  # 90% volume\n",
    "engine.say(\"Hello, how can I assist you today?\")\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7f762-8d18-4d90-ad1a-1511a22db8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = True\n",
    "print('BOT: My name is Robo, I will answer your questions about Machine Learning. If you want to exit, type Bye')\n",
    "\n",
    "interactions = [\n",
    "    'hi',\n",
    "    'what is Machine Learning?',\n",
    "    'What are the different types of Machine Learning algorithms?',\n",
    "    'What is the difference between supervised and unsupervised learning',\n",
    "    'What is the difference between classification and regression?',\n",
    "    'machine learning algorithms?',\n",
    "    'sounds awesome',\n",
    "    'bye',\n",
    "]\n",
    "\n",
    "sentence_tokens = []\n",
    "\n",
    "while flag:\n",
    "    user_response = input(\"User: \")\n",
    "    user_response = user_response.lower()\n",
    "\n",
    "    # Prevent adding duplicate questions to sentence_tokens\n",
    "    if user_response not in sentence_tokens:\n",
    "        sentence_tokens.append(user_response)\n",
    "\n",
    "    if user_response != 'bye':\n",
    "        if user_response == 'thanks' or user_response == 'thank you':\n",
    "            flag = False\n",
    "            print('BOT: You are welcome...')\n",
    "        elif greeting(user_response) != None:\n",
    "            print('ROBO: {}'.format(greeting(user_response)))\n",
    "            #engine.say(format(greeting(user_response)))\n",
    "            #engine.runAndWait()\n",
    "        else:\n",
    "            print('ROBO: ', end='')\n",
    "            print(response(user_response))\n",
    "            #engine.say(format(response(user_response)))\n",
    "            #engine.runAndWait()\n",
    "    else:\n",
    "        flag = False\n",
    "        print('BOT: bye!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8b46b-54f0-4dd8-b4dd-1f596b445d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
